import numpy as np
import pandas as pd
import datetime
from matplotlib import pyplot as plt
%matplotlib inline


from sklearn.linear_model import HuberRegressor
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.decomposition import PCA


from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import GroupKFold



pd.options.display.max_rows = 10
pd.options.display.max_colwidth = 100
pd.options.display.max_columns = 600
from tqdm import tqdm
import gc





def init():
    np.random.seed = 0
    
init()


# We fix the max xize to 181
# 2015 had 181 days with -3

max_size = 181 

# Offset value set 
offset = 1/2

# Read training data (train 2 csv)
train_all = pd.read_csv("../input/train_2.csv")

# Checking how entoire read data looks like
# head prints all of it
train_all.head()

# Entire training set copiied to all_page for manipulations
all_page = train_all.Page.copy()

# Copy the PAge from a ll training
train_key = train_all[['Page']].copy()

# multiply all by offset, multiply by half -  all except forst col
train_all = train_all.iloc[:,1:] * offset 

#show
train_all.head()

# If wanna see number of inx on particular date
def get_index(date, train_all=train_all):
    for idx, c in enumerate(train_all.columns):
        if date == c:
            break
    if idx == len(train_all.columns):
        return None
    return idx
    
# end point of training     
# training end index ix = get index for this date 10/09/2016 
trainEndIx = get_index('2016-09-10') + 1
testStartIx = get_index('2016-09-13')

#In entire training set start to end  , make its type float 32
#  Also #TODO .iLoc vs loc
# TODO End-max : end
# start : 63+start
# flota 32!
train = train_all.iloc[ : , (train_end - max_size) : train_end].copy().astype('float32')
# 2016-03-14 to 2016-09-10

# For test rest of set
test = train_all.iloc[:, test_start : (63 + test_start)].copy().astype('float32')
train = train.iloc[:,::-1].copy().astype('float32')

train_all = train_all.iloc[:,-(max_size):].astype('float32')
train_all = train_all.iloc[:,::-1].copy().astype('float32')

test_3_date = test.columns

# Splitting page name
# into PageTitle	Site	AccessAgent
data = [page.split('_') for page in tqdm(train_key.Page)]
access = ['_'.join(page[-2:]) for page in data]
site = [page[-3] for page in data]
page = ['_'.join(page[:-3]) for page in data]
train_key['PageTitle'] = page
train_key['Site'] = site
train_key['AccessAgent'] = access





# Return the natural logarithm of one plus the input array, element-wise.
# For real-valued input data types, log1p always returns real output.
# For each value that cannot be expressed as a real number or infinity, it yields
# nan and sets the invalid floating point error flag.

normalizedTraining = np.log1p(train).astype('float32')
normalizedTraining.head()
# values change a bit when done on all
normalizedTrainingAll = np.log1p(train_all).astype('float32')
normalizedTrainingAll.head()


first_day = 1 # 2016-09-13 is a Tuesday
test_columns_date = list(test.columns)
test_columns_code = ['w%d_d%d' % (i // 7, (first_day + i) % 7) for i in range(63)]
test.columns = test_columns_code

test.head()

test.fillna(0, inplace=True)

test['Page'] = all_page
test.sort_values(by='Page', inplace=True)
test.reset_index(drop=True, inplace=True)

test = test.merge(train_key, how='left', on='Page', copy=False)

test.head()


test_all_id = pd.read_csv('../input/key_2.csv')

test_all_id['Date'] = [page[-10:] for page in tqdm(test_all_id.Page)]
test_all_id['Page'] = [page[:-11] for page in tqdm(test_all_id.Page)]
test_all_id.head()







test_all = test_all_id.drop('Id', axis=1)
test_all['Visits_true'] = np.NaN

test_all.Visits_true = test_all.Visits_true * offset
test_all = test_all.pivot(index='Page', columns='Date', values='Visits_true').astype('float32').reset_index()

test_all['2017-11-14'] = np.NaN
test_all.sort_values(by='Page', inplace=True)
test_all.reset_index(drop=True, inplace=True)

test_all.head()





test_all.shape






test_all_columns_date = list(test_all.columns[1:])
first_day = 2 # 2017-13-09 is a Wednesday
test_all_columns_code = ['w%d_d%d' % (i // 7, (first_day + i) % 7) for i in range(63)]
cols = ['Page']
cols.extend(test_all_columns_code)
test_all.columns = cols
test_all.head()






test_all = test_all.merge(train_key, how='left', on='Page')
test_all.head()








y_cols = test.columns[:63]
y_cols





test = test.reset_index()
test_all = test_all.reset_index()







test_all.shape






test.shape






test.head()






test_all = test_all[test.columns].copy()
test_all.head()




train_cols = ['d_%d' % i for i in range(train_norm.shape[1])]
len(train_cols)






train_norm.columns = train_cols
train_all_norm.columns = train_cols






train_norm.head()






all(test[:test_all.shape[0]].Page == test_all.Page)





sites = train_key.Site.unique()
sites






test_site = pd.factorize(test.Site)[0]
test['Site_label'] = test_site
test_all['Site_label'] = test_site[:test_all.shape[0]]

accesses = train_key.AccessAgent.unique()
test_access = pd.factorize(test.AccessAgent)[0]
test['Access_label'] = test_access
test_all['Access_label'] = test_access[:test_all.shape[0]]


test.shape

test_all.shape

test0 = test.copy()
test_all0 = test_all.copy()

y_norm_cols = [c+'_norm' for c in y_cols]
var_rediction_Y_cols = [c+'_pred' for c in y_cols]

# all visits is median
# add median
# 1. max period * 7 - get that data
# 2. Copy page key a nd 
# 3. Put all visits in adf and all visits is actual median of training set - col wise, putting na =0
# 4. merge test
# 5. Make float
# 6. Loop throgh sites access and pages
#
def add_median(test, train,
               train_key, periods, max_periods, first_train_weekday):
    train =  train.iloc[:,:7*max_periods]
    
    df = train_key[['Page']].copy()
    df['AllVisits'] = train.median(axis=1).fillna(0)
    test = test.merge(df, how='left', on='Page', copy=False)
    test.AllVisits = test.AllVisits.fillna(0).astype('float32')
    
    for site in sites:
        test[site] = (1 * (test.Site == site)).astype('float32')
    
    for access in accesses:
        test[access] = (1 * (test.AccessAgent == access)).astype('float32')

    for (w1, w2) in periods:
        
        df = train_key[['Page']].copy()
        c = 'median_%d_%d' % (w1, w2)
        df[c] = train.iloc[:,7*w1:7*w2].median(axis=1, skipna=True) 
        test = test.merge(df, how='left', on='Page', copy=False)
        test[c] = (test[c] - test.AllVisits).fillna(0).astype('float32')

    for c_norm, c in zip(y_norm_cols, y_cols):
        test[c_norm] = (np.log1p(test[c]) - test.AllVisits).astype('float32')

    gc.collect()

    return test

max_periods = 16
periods = [(0,1), (1,2), (2,3), (3,4), 
           (4,5), (5,6), (6,7), (7,8),
           ]


site_cols = list(sites)
access_cols = list(accesses)

test, test_all = test0.copy(), test_all0.copy()

for c in var_rediction_Y_cols:
    test[c] = np.NaN
    test_all[c] = np.NaN

test1 = add_median(test, train_norm, 
                   train_key, periods, max_periods, 3)

test_all1 = add_median(test_all, train_all_norm, 
                       train_key, periods, max_periods, 5)
                       
num_cols = (['median_%d_%d' % (w1,w2) for (w1,w2) in periods])

import keras.backend as K

def metricErr(y_true, var_rediction_Y):
	# meanOf.(clipping.(absoluteOf.(y' - y)
    return K.mean(K.clip(K.abs(var_rediction_Y - y_true),  0.0, 1.0), axis=-1)

# GET MODEL!!

from keras.layers.normalization import BatchNormalization

from keras.layers import Input, Embedding, Dense, Activation, Dropout, Flatten
from keras.models import Sequential, Model
from keras import regularizers 
import keras

def gettingTheModel(input_dim, num_sites, num_accesses, output_dim):
    # seting meta drop out 0.5
    # regularizer initial 0.00004
    dropout = 0.5
    regularizer = 0.00004
    
    # Main input vs site input vs access input
    main_input = Input(shape=(input_dim,), dtype='float32', name='main_input')
    site_input = Input(shape=(num_sites,), dtype='float32', name='site_input')
    access_input = Input(shape=(num_accesses,), dtype='float32', name='access_input')
    
    # keras layers concat!!!
    x0 = keras.layers.concatenate([main_input, site_input, access_input])
    # RELU ACTIVATION fxn 
    x = Dense(200, activation='relu', 
              kernel_initializer='lecun_uniform', kernel_regularizer=regularizers.l2(regularizer))(x0)
			  #Drop val
    x = Dropout(dropout)(x)
    x = keras.layers.concatenate([x0, x])
	# Dnse , act  = rel9u
	# kernel initializer ' ;ecun
    x = Dense(200, activation='relu', 
              kernel_initializer='lecun_uniform', kernel_regularizer=regularizers.l2(regularizer))(x)
              
              
    # Batch normalization          
    x = BatchNormalization(beta_regularizer=regularizers.l2(regularizer),
                           gamma_regularizer=regularizers.l2(regularizer)
                          )(x)
    x = Dropout(dropout)(x)
    x = Dense(100, activation='relu', 
              kernel_initializer='lecun_uniform', kernel_regularizer=regularizers.l2(regularizer))(x)
    x = Dropout(dropout)(x)

    x = Dense(200, activation='relu', 
              kernel_initializer='lecun_uniform', kernel_regularizer=regularizers.l2(regularizer))(x)
    x = Dropout(dropout)(x)
    x = Dense(output_dim, activation='linear', 
              kernel_initializer='lecun_uniform', kernel_regularizer=regularizers.l2(regularizer))(x)
    # Model ready
    model =  Model(inputs=[main_input, site_input, access_input], outputs=[x])
    # compile adam optimizer
	# compile ya model
	# defining a loss based on eval metric 
    model.compile(loss=metricErr, optimizer='adam')
    return model

# AMke a group
# Pandas factorization
# On test data [0]	
group = pd.factorize(test1.Page)[0]


# initialize bagging no
n_bag = 20

# K fold grouping here! on the bagging #
kf = GroupKFold(n_bag)


# Set batch 
batch_size=4096

# add to 2

testNewOf1 = test1
testNew2 = test_all1



X, Xs, Xa, y = testNewOf1[num_cols].values, testNewOf1[site_cols].values, testNewOf1[access_cols].values, testNewOf1[y_norm_cols].values
X_all, Xs_all, Xa_all, y_all = testNew2[num_cols].values, testNew2[site_cols].values, testNew2[access_cols].values, testNew2[y_norm_cols].fillna(0).values

y_true = test2[y_cols]
y_all_true = testNew2[y_cols]

models = [gettingTheModel(len(num_cols), len(site_cols), len(access_cols), len(y_cols)) for bag in range(n_bag)]

best_score = 100
best_all_score = 100
save_pred = 0
saved_pred_all = 0

for n_epoch in range(10, 201, 10):
    #Startin epochs

    var_rediction_Y0 = np.zeros((y.shape[0], y.shape[1]))
    var_rediction_all_Y0 = np.zeros((n_bag, y_all.shape[0], y_all.shape[1]))
    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y, group)):
 
        model = models[fold]
        X_train, Xs_train, Xa_train, y_train = X[train_idx,:], Xs[train_idx,:], Xa[train_idx,:], y[train_idx,:]
        X_test, Xs_test, Xa_test, y_test = X[test_idx,:], Xs[test_idx,:], Xa[test_idx,:], y[test_idx,:]

        model.fit([ X_train, Xs_train, Xa_train],  y_train, 
                  epochs=10, batch_size=batch_size, verbose=0, shuffle=True, 
                  #validation_data=([X_test, Xs_test, Xa_test],  y_test)
                 )
        var_rediction_Y = model.predict([ X_test, Xs_test, Xa_test], batch_size=batch_size)
        var_rediction_all_Y = model.predict([X_all, Xs_all, Xa_all], batch_size=batch_size)

        var_rediction_Y0[test_idx,:] = var_rediction_Y
        var_rediction_all_Y0[fold,:,:]  = var_rediction_all_Y

        var_rediction_Y += test2.AllVisits.values[test_idx].reshape((-1,1))
        var_rediction_Y = np.expm1(var_rediction_Y)
        var_rediction_Y[var_rediction_Y < 0.5 * offset] = 0
        res = metricTWO(test2[y_cols].values[test_idx, :], var_rediction_Y)
        var_rediction_Y = offset*((var_rediction_Y / offset).round())
        res_round = metricTWO(test2[y_cols].values[test_idx, :], var_rediction_Y)

        var_rediction_all_Y += test_all2.AllVisits.values.reshape((-1,1))
        var_rediction_all_Y = np.expm1(var_rediction_all_Y)
        var_rediction_all_Y[var_rediction_all_Y < 0.5 * offset] = 0
        resultAll = metricTWO(test_all2[y_cols], var_rediction_all_Y)
        var_rediction_all_Y = offset*((var_rediction_all_Y / offset).round())
        resultAll_round = metricTWO(test_all2[y_cols], var_rediction_all_Y)


    var_rediction_all_Y0  = np.nanmedian(var_rediction_all_Y0, axis=0)

    var_rediction_Y0  += test2.AllVisits.values.reshape((-1,1))
    var_rediction_Y0 = np.expm1(var_rediction_Y0)
    var_rediction_Y0[var_rediction_Y0 < 0.5 * offset] = 0
    res = metricTWO(y_true, var_rediction_Y0)

    var_rediction_Y0 = offset*((var_rediction_Y0 / offset).round())
    res_round = metricTWO(y_true, var_rediction_Y0)


    var_rediction_all_Y0 += test_all2.AllVisits.values.reshape((-1,1))
    var_rediction_all_Y0 = np.expm1(var_rediction_all_Y0)
    var_rediction_all_Y0[var_rediction_all_Y0 < 0.5 * offset] = 0
    #var_rediction_all_Y0 = var_rediction_all_Y0.round()
    resultAll = smape2D(y_all_true, var_rediction_all_Y0)
 
    var_rediction_all_Y0 = offset*((var_rediction_all_Y0 / offset).round())
    resultAll_round = smape2D(y_all_true, var_rediction_all_Y0)

    if res_round < best_score:
        best_score = res_round
        best_all_score = resultAll_round
        test.loc[:, var_rediction_Y_cols] = var_rediction_Y0
        test_all.loc[:, var_rediction_Y_cols] = var_rediction_all_Y0
    else:
        print()

print('besscore:', best_all_score)

# Calc metric to return diff
# Sum = numpy absolute value of actual outputs + numpy absolute value of predicted outputs
# DivieBy = Sum/ 2
# Difference  =  numpy absolute value of (actual outputs - predicted outputs) / DiviDey
# difference[ DidvideBy == 0] = 0
# Mean of Difference
# In short:
#   return  ----- meanOf( [y-y'][(y + y')/ 2] ) for all outputs in training set
def retTrainingMetric(y_true, var_rediction_Y):
    denominator = (np.abs(y_true) + np.abs(var_rediction_Y)) / 2.0
    diff = np.abs(y_true - var_rediction_Y) / denominator
    diff[denominator == 0] = 0.0
    return np.nanmean(diff)
    
    
# TODO np.ravel
def metricTWO(y_true, var_rediction_Y):
    return v(np.ravel(y_true), np.ravel(var_rediction_Y))

# Masking of mean on a threshold
def smape_mask(y_true, var_rediction_Y, threshold):
    # Divide By  =  y + y '
    # Diff : y - y'
    # [y - y '] [y + y ' == 0 ] = 0
    # y-y' < = (threshol/2) * (y + y')
    denominator = (np.abs(y_true) + np.abs(var_rediction_Y)) 
    diff = np.abs(y_true - var_rediction_Y) 
    diff[denominator == 0] = 0.0
    
    return diff <= (threshold / 2.0) * denominator
    
    
    
    
